{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f02f824fbb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w와 편향 b 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563628\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497595\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435044\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375726\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319507\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215703\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167810\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079390\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  80], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((3,1),requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :    0 / 20,hypothesis : tensor([154.0294, 185.0676, 175.9234, 198.5185, 141.2278]) , cost : 5.536077\n",
      "epoch :    1 / 20,hypothesis : tensor([154.0292, 185.0673, 175.9245, 198.5179, 141.2278]) , cost : 5.533695\n",
      "epoch :    2 / 20,hypothesis : tensor([154.0291, 185.0671, 175.9255, 198.5174, 141.2279]) , cost : 5.531332\n",
      "epoch :    3 / 20,hypothesis : tensor([154.0289, 185.0668, 175.9265, 198.5168, 141.2280]) , cost : 5.528962\n",
      "epoch :    4 / 20,hypothesis : tensor([154.0287, 185.0665, 175.9275, 198.5163, 141.2280]) , cost : 5.526603\n",
      "epoch :    5 / 20,hypothesis : tensor([154.0286, 185.0663, 175.9285, 198.5157, 141.2281]) , cost : 5.524244\n",
      "epoch :    6 / 20,hypothesis : tensor([154.0284, 185.0660, 175.9295, 198.5151, 141.2282]) , cost : 5.521854\n",
      "epoch :    7 / 20,hypothesis : tensor([154.0283, 185.0657, 175.9305, 198.5145, 141.2282]) , cost : 5.519456\n",
      "epoch :    8 / 20,hypothesis : tensor([154.0281, 185.0654, 175.9315, 198.5140, 141.2283]) , cost : 5.517091\n",
      "epoch :    9 / 20,hypothesis : tensor([154.0280, 185.0652, 175.9325, 198.5134, 141.2284]) , cost : 5.514731\n",
      "epoch :   10 / 20,hypothesis : tensor([154.0278, 185.0649, 175.9335, 198.5128, 141.2284]) , cost : 5.512386\n",
      "epoch :   11 / 20,hypothesis : tensor([154.0276, 185.0646, 175.9346, 198.5123, 141.2285]) , cost : 5.510032\n",
      "epoch :   12 / 20,hypothesis : tensor([154.0275, 185.0643, 175.9356, 198.5117, 141.2286]) , cost : 5.507647\n",
      "epoch :   13 / 20,hypothesis : tensor([154.0273, 185.0641, 175.9366, 198.5111, 141.2286]) , cost : 5.505295\n",
      "epoch :   14 / 20,hypothesis : tensor([154.0272, 185.0638, 175.9376, 198.5106, 141.2287]) , cost : 5.502926\n",
      "epoch :   15 / 20,hypothesis : tensor([154.0270, 185.0635, 175.9386, 198.5100, 141.2288]) , cost : 5.500561\n",
      "epoch :   16 / 20,hypothesis : tensor([154.0269, 185.0633, 175.9396, 198.5095, 141.2288]) , cost : 5.498231\n",
      "epoch :   17 / 20,hypothesis : tensor([154.0267, 185.0630, 175.9406, 198.5089, 141.2289]) , cost : 5.495840\n",
      "epoch :   18 / 20,hypothesis : tensor([154.0266, 185.0628, 175.9416, 198.5083, 141.2290]) , cost : 5.493528\n",
      "epoch :   19 / 20,hypothesis : tensor([154.0264, 185.0625, 175.9426, 198.5078, 141.2290]) , cost : 5.491187\n",
      "epoch :   20 / 20,hypothesis : tensor([154.0262, 185.0622, 175.9436, 198.5072, 141.2291]) , cost : 5.488810\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD([W,b] , lr= 1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    hypothesis = x_train.matmul(W) + b # matmul 은 내적\n",
    "\n",
    "    cost = torch.mean((y_train - hypothesis)**2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'epoch : {epoch:>4d} / {nb_epochs},hypothesis : {hypothesis.squeeze().detach()} , cost : {cost:6.6f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haha",
   "language": "python",
   "name": "haha"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
